<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Satyanarayan Rao - Concepts</title><link href="/" rel="alternate"></link><link href="/feeds/concepts.atom.xml" rel="self"></link><id>/</id><updated>2018-03-09T00:00:00+01:00</updated><entry><title>Bayesian inference to L2-regularized MLR</title><link href="/bayesian-inference-to-l2-regularized-mlr.html" rel="alternate"></link><published>2018-03-09T00:00:00+01:00</published><updated>2018-03-09T00:00:00+01:00</updated><author><name>Satya</name></author><id>tag:None,2018-03-09:/bayesian-inference-to-l2-regularized-mlr.html</id><summary type="html">&lt;p&gt;I was really happy to read the chapter 1 of Bishop's Pattern Recognition book
and realizing the relationship between L2-regularized MLR and Bayesian
perspective. On &lt;code&gt;page 30&lt;/code&gt;, he shows that if weights follow a
gaussian distribution conditioned on selected hyper-parameter, then the maximum likelihood function would take the following form â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;I was really happy to read the chapter 1 of Bishop's Pattern Recognition book
and realizing the relationship between L2-regularized MLR and Bayesian
perspective. On &lt;code&gt;page 30&lt;/code&gt;, he shows that if weights follow a
gaussian distribution conditioned on selected hyper-parameter, then the maximum likelihood function would take the following form: &lt;/p&gt;
&lt;p&gt;$$ \frac{\beta}{2}\sum\limits_{n = 1}^{N} {y(x_{n}, \mathbf{w}) - t_{n}}^2 + \frac{\alpha}{2}\mathbf{w}^T\mathbf{w} $$&lt;/p&gt;
&lt;p&gt;Which is similar to L2-regularized MLR with: &lt;/p&gt;
&lt;p&gt;$$ \lambda = \frac{\alpha}{\beta}$$&lt;/p&gt;
&lt;p&gt;NOTE: This finding was a result of healthy discussion with &lt;a href="https://github.com/TsuPeiChiu"&gt;Tsu-Pei&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;A good question asked in this context was:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Why do I need to bother about the distribution in cases where the length of weight vector $\mathbf{w}$ very small, for example, 5 or less than 10. &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And my response to this questions was: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Well, probably there may not be any need of regularization in these type of cases. Ideally, we use regularization when we have considerable number of features. &lt;/li&gt;
&lt;/ul&gt;</content></entry></feed>