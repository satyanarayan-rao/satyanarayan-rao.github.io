<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Satyanarayan Rao - Concepts</title><link href="/" rel="alternate"></link><link href="/feeds/concepts.atom.xml" rel="self"></link><id>/</id><updated>2018-04-15T00:00:00+02:00</updated><entry><title>Linear Models</title><link href="/linear-models.html" rel="alternate"></link><published>2018-04-15T00:00:00+02:00</published><updated>2018-04-15T00:00:00+02:00</updated><author><name>Satya</name></author><id>tag:None,2018-04-15:/linear-models.html</id><summary type="html">&lt;h1&gt;Linear model?&lt;/h1&gt;
&lt;p&gt;Again referring the chapter #1 of Bishop's books, where describes some
fundamental concepts about linearity. &lt;/p&gt;
&lt;h2&gt;Two apsects&lt;/h2&gt;</summary><content type="html">&lt;h1&gt;Linear model?&lt;/h1&gt;
&lt;p&gt;Again referring the chapter #1 of Bishop's books, where describes some
fundamental concepts about linearity. &lt;/p&gt;
&lt;h2&gt;Two apsects&lt;/h2&gt;</content></entry><entry><title>Bayesian inference to L2-regularized MLR</title><link href="/bayesian-inference-to-l2-regularized-mlr.html" rel="alternate"></link><published>2018-03-09T00:00:00+01:00</published><updated>2018-03-09T00:00:00+01:00</updated><author><name>Satya</name></author><id>tag:None,2018-03-09:/bayesian-inference-to-l2-regularized-mlr.html</id><summary type="html">&lt;p&gt;I was really happy to read the chapter 1 of Bishop's Pattern Recognition book
and realize the relationship between L2-regularized MLR and Bayesian
perspective. On &lt;code&gt;page 30&lt;/code&gt;, he shows that if weights follow a gaussian
distribution conditioned on selected hyper-parameter, then the maximum
likelihood would take the following form: &lt;/p&gt;
&lt;div class="math"&gt;$$ \frac â€¦&lt;/div&gt;</summary><content type="html">&lt;p&gt;I was really happy to read the chapter 1 of Bishop's Pattern Recognition book
and realize the relationship between L2-regularized MLR and Bayesian
perspective. On &lt;code&gt;page 30&lt;/code&gt;, he shows that if weights follow a gaussian
distribution conditioned on selected hyper-parameter, then the maximum
likelihood would take the following form: &lt;/p&gt;
&lt;div class="math"&gt;$$ \frac{\beta}{2}\sum\limits_{n = 1}^{N} \{y(x_{n}, \mathbf{w}) - t_{n}\}^2 + \frac{\alpha}{2}\mathbf{w}^T\mathbf{w} $$&lt;/div&gt;
&lt;p&gt;Which is similar to L2-regularized MLR with: &lt;/p&gt;
&lt;div class="math"&gt;$$ \lambda = \frac{\alpha}{\beta}$$&lt;/div&gt;
&lt;p&gt;NOTE: This finding was a result of healthy discussion with &lt;a href="https://github.com/TsuPeiChiu"&gt;Tsu-Pei&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;A good question asked in this context was:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Why do I need to bother about the distribution in cases where the length of weight vector &lt;span class="math"&gt;\(\mathbf{w}\)&lt;/span&gt; very small, for example, 5 or less than 10. &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And my response to this questions was: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Well, probably there may not be any need of regularization in these type of cases. Ideally, we use regularization when we have considerable number of features. &lt;/li&gt;
&lt;/ul&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content></entry></feed>